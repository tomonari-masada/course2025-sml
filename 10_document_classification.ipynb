{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ud7V89xZ504GreKsw6yTFAkk9U45S3FR",
      "authorship_tag": "ABX9TyOpEJQEQRL6jaagt9Ciifuh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2025-sml/blob/main/10_document_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77taeHZLUaTm"
      },
      "source": [
        "# 文書分類\n",
        "\n",
        "* IMDBデータセットを使った感情分析\n",
        "  * 映画レビューを、映画に肯定的か否定的かで、2値に分類する。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 文書分類の目的\n",
        "  1. 検証データでできるかぎりチューニングを行い、最後にテストデータでの分類性能を明らかにする。\n",
        "\n",
        "  2. 肯定的なレビューと否定的なレビューとを分類する際に、どのような単語が特に効いているか、明らかにする。"
      ],
      "metadata": {
        "id": "AKQxOXSt6vjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "voyWEzd9gEV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Faceの`datasets`ライブラリのインストール\n",
        "* 今回は、IMDBデータセットを取得するためだけに使う。"
      ],
      "metadata": {
        "id": "ZaqOhxf4gFaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "BeJTsj0JN0Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMDBデータセットの取得"
      ],
      "metadata": {
        "id": "3VKtbS-FgPQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"stanfordnlp/imdb\")"
      ],
      "metadata": {
        "id": "KbLyDRmMN2Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"]"
      ],
      "metadata": {
        "id": "VUSCpp4A-5lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"text\"][0]"
      ],
      "metadata": {
        "id": "nQTUbvWRN-PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"label\"][:10]"
      ],
      "metadata": {
        "id": "tXpefzTLASE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"test\"]"
      ],
      "metadata": {
        "id": "Q2vi6-Vo-71O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 後で扱いやすいようにNumPyの配列に変換しておく。"
      ],
      "metadata": {
        "id": "q4akjr_Uf-OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_corpus = np.array(dataset[\"train\"][\"text\"])\n",
        "y_train = np.array(dataset[\"train\"][\"label\"])\n",
        "\n",
        "test_corpus = np.array(dataset[\"test\"][\"text\"])\n",
        "y_test = np.array(dataset[\"test\"][\"label\"])"
      ],
      "metadata": {
        "id": "qQRUbdjzf5E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### インポート"
      ],
      "metadata": {
        "id": "t-XpkYDXgSaM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qos6P0RsSsqg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKg-InjbVh7r"
      },
      "source": [
        "## TF-IDFによるテキストの埋め込み\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQWEdxwuC7Tt"
      },
      "source": [
        "### TF-IDFとは\n",
        "* TF-IDFはテキストをベクトル化する古典的な方法。\n",
        "* ベクトルの次元は語彙数となる。\n",
        "\n",
        "  * すべての単語について、テキストごとに重みを計算する方法。\n",
        "  * TFとは、あるテキストのなかにその単語が何回出現するか、その回数。\n",
        "  * DFとは、単語が出現するテキストの数。IDFはDFの逆数。\n",
        "  * TF-IDFは、ざっくり言うと、TFとIDFの積。\n",
        "* 特定のテキストに注目すると・・・\n",
        "  * そのテキストに出現する回数が多い単語ほど、TF-IDFは大きい。\n",
        "  * しかし、多数のテキストに出現する単語は、IDFが小さくなるので・・・\n",
        "  * そのテキストで頻出していても、TF-IDFは小さくなる。\n",
        "* scikit-learnの`TfidfVectorizer`で簡単に計算できる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbLfK3LvDEMf"
      },
      "source": [
        "### `TfidfVectorizer`の使い方\n",
        "* 訓練セットだけでfitすること。検証セットやテストセットはtransformするだけ。\n",
        "\n",
        "* `stop_words='english'`とは、英語のストップワードは語彙から取り除く、という意味。\n",
        "  * ストップワードとはthe, a, is, ofなど、ありふれすぎていて、文書分類などのタスクにはあまり効かない単語のこと。こういう単語は、特徴量削減の意味も含めて、しばしばあらかじめ削除しておく。\n",
        "\n",
        "* `min_df`は、0から1の間の実数で指定すると、その割合より少ない文書にしか出現しない単語を削除する、という意味のパラメータ。\n",
        "  * 希少な単語を削除するために使う。\n",
        "\n",
        "* `max_df`は、0から1の間の実数で指定すると、その割合より多い文書に出現する単語を削除する、という意味のパラメータ。\n",
        "  * ありふれた単語を削除するために使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHsek-uzTxi0"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(dataset[\"train\"][\"text\"])\n",
        "print('X_train: 文書数 {}, 語彙数 {}'.format(*X_train.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy5RKu8zDZVP"
      },
      "source": [
        "* 語彙を取得する。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUcTnazzT5Qm"
      },
      "source": [
        "vocab = np.array(vectorizer.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70HQNALpDcQl"
      },
      "source": [
        "* 語彙の一部を見てみる（アルファベット順に並んでいるようだ）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e2QrURgT_gr"
      },
      "source": [
        "print(vocab[500:520])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YApqwsSrDeJK"
      },
      "source": [
        "* 最初の文書をTF-IDFでベクトル化したらどうなったかを、見てみる。\n",
        "  * スパースな表現になっている。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GZx-gLj2QRf"
      },
      "source": [
        "print(type(X_train))\n",
        "print(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmRAzGE1W4af"
      },
      "source": [
        "## ロジスティック回帰のチューニング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlKIK8jlT_SO"
      },
      "source": [
        "### L2正則化\n",
        "* 交差検証で正則化パラメータをチューニングする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMBWsc8wS_9o"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "for C in 10. ** np.arange(-1, 4):\n",
        "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
        "  scores = []\n",
        "  for train_index, valid_index in skf.split(train_corpus, y_train):\n",
        "    X_train = vectorizer.fit_transform(train_corpus[train_index])\n",
        "    clf = LogisticRegression(C=C, solver='liblinear', random_state=123)\n",
        "    clf.fit(X_train, y_train[train_index])\n",
        "    X_valid = vectorizer.transform(train_corpus[valid_index])\n",
        "    score = clf.score(X_valid, y_train[valid_index])\n",
        "    print(f\"\\t{score:.3f}\", end=\" \")\n",
        "    scores.append(score)\n",
        "  print(f\"\\nmean accuracy: {np.array(scores).mean():.3f} for C={C:.2e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOOvEgPQVqVI"
      },
      "source": [
        "### L1正則化\n",
        "* 交差検証で正則化パラメータをチューニングする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_K4tVR6Vr5E"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "for C in 10. ** np.arange(-1, 4):\n",
        "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
        "  scores = []\n",
        "  for train_index, valid_index in skf.split(train_corpus, y_train):\n",
        "    X_train = vectorizer.fit_transform(train_corpus[train_index])\n",
        "    clf = LogisticRegression(penalty='l1', C=C, solver='liblinear', random_state=123)\n",
        "    clf.fit(X_train, y_train[train_index])\n",
        "    X_valid = vectorizer.transform(train_corpus[valid_index])\n",
        "    score = clf.score(X_valid, y_train[valid_index])\n",
        "    print(f\"\\t{score:.3f}\", end=\" \")\n",
        "    scores.append(score)\n",
        "  print(f\"\\nmean accuracy: {np.array(scores).mean():.3f} for C={C:.2e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "190x80QqULfi"
      },
      "source": [
        "## TF-IDFのチューニング\n",
        "* `min_df`を変えてみる。\n",
        "  * `max_df`の設定も変えてよい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvSnZVQ-T-Lx"
      },
      "source": [
        "for min_df in [0.0, 1/5000, 1/2000, 1/1000]:\n",
        "\n",
        "  vectorizer = TfidfVectorizer(stop_words='english', min_df=min_df)\n",
        "\n",
        "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
        "  scores = []\n",
        "  for train_index, valid_index in skf.split(train_corpus, y_train):\n",
        "    X_train = vectorizer.fit_transform(train_corpus[train_index])\n",
        "    clf = LogisticRegression(C=1.0, solver='liblinear', random_state=123)\n",
        "    clf.fit(X_train, y_train[train_index])\n",
        "    X_valid = vectorizer.transform(train_corpus[valid_index])\n",
        "    score = clf.score(X_valid, y_train[valid_index])\n",
        "    print(f\"\\t{score:.3f}\", end=\" \")\n",
        "    scores.append(score)\n",
        "  print(f\"\\nmean accuracy: {np.array(scores).mean():.3f} for min_df={min_df}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWcZHyJXFzR"
      },
      "source": [
        "## テストセットで評価\n",
        "\n",
        "* LogisticRegression() のカッコ内には、自分で見つけたベストの設定を書き込む。\n",
        "* 学習は、訓練データ全体（テストデータ以外の全体）を使っている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQE3CxUdfZ7V"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "\n",
        "clf = LogisticRegression(C=1.0, solver=\"liblinear\", random_state=123)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "X_test = vectorizer.transform(test_corpus)\n",
        "score = clf.score(X_test, y_test)\n",
        "print(f\"test accuracy: {score:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RocCurveDisplay.from_estimator(clf, X_test, y_test, name=\"logistic regression\");"
      ],
      "metadata": {
        "id": "nCk_faR6SkVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PrecisionRecallDisplay.from_estimator(clf, X_test, y_test, name=\"logistic regression\");"
      ],
      "metadata": {
        "id": "EsdkLsBOSmrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdLcpwSwf3s3"
      },
      "source": [
        "## SVM\n",
        "\n",
        "* SVMについては、私からは実行例を示しません。各自、試行錯誤してください。"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "awbGsdI0kCxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CtVT3KEXM7f"
      },
      "source": [
        "## ロジスティック回帰による分類に効いている単語を調べる\n",
        "\n",
        "* 下に示すのは、あくまで一つの方法にすぎないです。他にどんな方法があるか調べて使ってみよう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGd-_fRQqQXO"
      },
      "source": [
        "### ロジスティック回帰の係数をそのまま可視化する\n",
        "\n",
        "* 正の係数と負の係数で、それぞれ絶対値が大きいもの30個を可視化する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rIu7hTNmlp7"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "clf = LogisticRegression(C=1.0, solver='liblinear', random_state=123)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "n_features = 30\n",
        "positions = np.arange(n_features)[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g-CNIfBIya6"
      },
      "source": [
        "* 肯定的なレビューへの分類に効いている単語"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHLsDApX3fng"
      },
      "source": [
        "indices = np.argsort(- clf.coef_[0])[:n_features]\n",
        "widths = clf.coef_[0][indices]\n",
        "yticks = vocab[indices]\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.barh(positions, widths, align='center')\n",
        "plt.yticks(positions, yticks)\n",
        "plt.xlabel('coefficients')\n",
        "plt.title(f'Coefficients of {n_features} Features Using Logistic Regression');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKLvtUPZJAjf"
      },
      "source": [
        "* 否定的なレビューへの分類に効いている単語"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K_Zv4UbkEaM"
      },
      "source": [
        "indices = np.argsort(clf.coef_[0])[:n_features]\n",
        "widths = clf.coef_[0][indices]\n",
        "yticks = vocab[indices]\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.barh(positions, widths, align='center')\n",
        "plt.yticks(positions, yticks)\n",
        "plt.xlabel('coefficients')\n",
        "plt.title(f'Coefficients of {n_features} Features Using Logistic Regression');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzx5_vC5JnA_"
      },
      "source": [
        "## SVMによる分類に効いている単語を調べる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LSWP9vldWJu"
      },
      "source": [
        "* SVMについては実行例を示しません。上を参考に各自試してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yrFh3vhnkEUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題\n",
        "* SVMによる分類（上で空欄にしてある部分）と分類器の解釈を実践してみよう。"
      ],
      "metadata": {
        "id": "FeOD8lMRnLRS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6LJk1O2nYKb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}